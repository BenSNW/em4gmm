em4gmm
======

Fast C implementation of the clustering Expectation Maximization (EM) algorithm for estimating Gaussian Mixture Models (GMMs).

Introduction
------------
In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.

For this reason EM is frequently used for data clustering, verification and identification of the speaker (biometrics), author profiling based on his documents, automatic document categorization, and many more applications.

Download
--------
* [Download the latest release of the source code on a zip file](https://github.com/juandavm/em4gmm/zipball/master).
* Clone the repository: `git clone git://github.com/juandavm/em4gmm.git`

Compiling
---------

In order to compile this program, you need first compile and install the [zlib library](http://www.zlib.net) from their website, or using your preferred software distribution channels (aptitude, yum, macports, etc) in order to install it (with the dev packages).

On Mac Os X and Linux distributions you can simple use the make command on the system shell to compile it, and then sudo make install to install it on yout system (by default on /usr/bin). We recommend the use of the lastest version of GCC compiler (because the code generated by LLVM is, for now, much slower).

Note that the default compilation makes this program work with 16 threads, if for any reason you want to change this value, feel free to edit the NUM_THREADS constant on the global.h file. Note that the minimum value of this constant must be 1.

Usage
-----

You can train a mixture model using the gmmtrain utility on a feature train file (described below):

     Usage: gmmtrain <options>
       -d file.txt|file.gz   file that contains all the samples vectors
       -m file.gmm           file used to save the trained mixture model
       -n 2-1048576          optional number of components of the mixture
       -s 0.0001-1.0         optional stop criterion based on likelihood
       -h                    optional argument that shows this message

Also, yo can obtain the score/loglikelihood of one feature test file (described below) using the gmmclass utility:

     Usage: gmmclass <options>
       -d file.txt|file.gz   file that contains the samples vectors
       -m file.gmm           file of the trained model used to classify
       -w file.gmm           optional world model used to smooth
       -r file.json          optional file to save the classify log
       -h                    optional argument that shows this message

The standard process is to train a model for each class, and then classify at the class with highest score.

You also can obtain a detailed analysis of the classify process using the "-r" option on the gmmclass utility. This option lets you to know the component assigned to each sample, and the occupation of the components. This large report is usefull to do automatic clustering of some data on a predetermined number of clusters.

Speed Results
-------------

Compiling with GCC 4.5.4 on Mac Os X (2,66GHz Intel Core 2 Duo) with the included gzip compressed features (100000 samples of 10 dimensions of decimal values):

     Mixtures     Training       Classify       Classify+WM
     32           1.19s          0.12s          0.14s
     64           2.25s          0.14s          0.22s
     128          5.76s          0.22s          0.32s
     256          15.31s         0.32s          0.61s
     512          21.02s         0.61s          1.11s

Data Files
----------

The data files used by this software are very simple. They are plain text files of decimal numbers, with a header, and one line per sample vector. This is an example of a very short data file:

     11       4
     1025     7706     6830     5571     4169     2858     1809     1094      688      500      417
     1147     5755     6636     6234     4118     4593     2750     3649      774     1568     1104
      932     5381     5567     5175     3613     3499     2429     2536      652      913      337
      838     6401     5961     5277     4418     3468     2516     1644      921      391       74

On the header, the first number are the dimension and the second the number of samples. The sample's vectors can be integers or decimals (using "." as separator), and the dimensions must be space-separated. Also, you have an example of a data file on the dat directory of this project.

If you want to save disk space you can compress data files using gzip format (.gz file). A simple way to do this compression is using the gzip linux or mac command.

Issues and Bugs
---------------
Do you have a bug or a feature request? Do not worry, [open a new issue](https://github.com/juandavm/em4gmm/issues). But please, before opening any new issue, search on existing the yours in order to avoid duplicates. And thanks you for your contribution!

Authors
------
 * [Juan Daniel Valor Miró](http://www.juandaniel.es/).

License
-------
     Expectation Maximization for Gaussian Mixture Models.
     Copyright (C) 2012-2013 Juan Daniel Valor Miro
     
     This program is free software; you can redistribute it and/or
     modify it under the terms of the GNU General Public License as
     published by the Free Software Foundation; either version 2 of
     the License, or (at your option) any later version.
     
     This program is distributed in the hope that it will be useful,
     but WITHOUT ANY WARRANTY; without even the implied warranty of
     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
     General Public License for more details.

